# üö¢ SEMMA Methodology: Titanic Survival Prediction

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue?style=for-the-badge&logo=python&logoColor=white" alt="Python">
  <img src="https://img.shields.io/badge/Scikit--Learn-1.3+-orange?style=for-the-badge&logo=scikit-learn&logoColor=white" alt="Scikit-learn">
  <img src="https://img.shields.io/badge/XGBoost-2.0+-green?style=for-the-badge&logo=xgboost&logoColor=white" alt="XGBoost">
  <img src="https://img.shields.io/badge/Jupyter-Notebook-orange?style=for-the-badge&logo=jupyter&logoColor=white" alt="Jupyter">
</p>

## üìã Descripci√≥n

Aplicaci√≥n completa de la **metodolog√≠a SEMMA** (Sample, Explore, Modify, Model, Assess) desarrollada por SAS Institute, utilizando el dataset Titanic de Kaggle para predecir la supervivencia de pasajeros.

Este proyecto demuestra un flujo de trabajo profesional de Data Science, desde la exploraci√≥n inicial hasta la evaluaci√≥n de m√∫ltiples modelos de Machine Learning.

---

## üéØ Objetivo

Predecir qu√© pasajeros sobrevivieron al naufragio del RMS Titanic en 1912 utilizando t√©cnicas de clasificaci√≥n supervisada, aplicando rigurosamente las 5 fases de la metodolog√≠a SEMMA.

---

## üìÅ Estructura del Proyecto

```
PRO_ANALISIS/
‚îú‚îÄ‚îÄ üìÇ data/
‚îÇ   ‚îî‚îÄ‚îÄ titanic.csv                    # Dataset original (891 registros)
‚îú‚îÄ‚îÄ üìÇ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ SEMMA_Titanic_Completo.ipynb   # Notebook principal con todo el an√°lisis
‚îú‚îÄ‚îÄ üìÇ outputs/
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ graficos/                   # 10 visualizaciones generadas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01_sample_distribution.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 02_explore_missing_values.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 03_explore_survival_analysis.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 04_explore_correlation_heatmap.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 05_explore_boxplots.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 06_modify_feature_engineering.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 07_assess_metrics_comparison.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 08_assess_confusion_matrices.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 09_assess_roc_curves.png
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 10_assess_feature_importance.png
‚îÇ   ‚îî‚îÄ‚îÄ üìÇ modelos/                    # Modelos entrenados (.pkl)
‚îÇ       ‚îú‚îÄ‚îÄ logistic_regression_model.pkl
‚îÇ       ‚îú‚îÄ‚îÄ random_forest_model.pkl
‚îÇ       ‚îú‚îÄ‚îÄ xgboost_model.pkl
‚îÇ       ‚îî‚îÄ‚îÄ scaler.pkl
‚îú‚îÄ‚îÄ üìÑ Informe_Investigacion_Formativa_SEMMA.docx  # Informe acad√©mico
‚îî‚îÄ‚îÄ üìÑ README.md
```

---

## üîÑ Metodolog√≠a SEMMA

### 1Ô∏è‚É£ SAMPLE (Muestreo)
| Acci√≥n | Detalle |
|--------|---------|
| Carga de datos | 891 registros con 12 variables |
| Divisi√≥n | 80% entrenamiento / 20% validaci√≥n |
| Estratificaci√≥n | Mantiene proporci√≥n de sobrevivientes en ambos conjuntos |

### 2Ô∏è‚É£ EXPLORE (Exploraci√≥n)
| An√°lisis | Hallazgo |
|----------|----------|
| Valores faltantes | Age (~20%), Cabin (~77%), Embarked (0.2%) |
| Supervivencia por g√©nero | Mujeres: ~74% vs Hombres: ~19% |
| Supervivencia por clase | 1ra: ~63%, 2da: ~47%, 3ra: ~24% |

### 3Ô∏è‚É£ MODIFY (Modificaci√≥n)
| Transformaci√≥n | Descripci√≥n |
|----------------|-------------|
| Imputaci√≥n Age | Mediana (28 a√±os) |
| Imputaci√≥n Embarked | Moda ('S' - Southampton) |
| FamilySize | SibSp + Parch + 1 |
| IsAlone | 1 si viaja solo, 0 si tiene familia |
| Title | Extra√≠do del nombre (Mr, Mrs, Miss, Master, Rare) |
| HasCabin | 1 si tiene cabina asignada |

### 4Ô∏è‚É£ MODEL (Modelado)
| Modelo | Configuraci√≥n |
|--------|---------------|
| **Logistic Regression** | Baseline, max_iter=1000 |
| **Random Forest** | GridSearchCV con optimizaci√≥n de hiperpar√°metros |
| **XGBoost** | 200 estimators, max_depth=5, learning_rate=0.1 |

### 5Ô∏è‚É£ ASSESS (Evaluaci√≥n)
| Modelo | Accuracy | Precision | Recall | F1-Score | AUC-ROC |
|--------|----------|-----------|--------|----------|---------|
| Logistic Regression | ~80% | ~76% | ~72% | ~74% | ~0.84 |
| Random Forest | ~82% | ~80% | ~73% | ~76% | **~0.87** |
| XGBoost | ~83% | ~78% | ~75% | ~77% | ~0.86 |

> üèÜ **Mejor Modelo:** Random Forest con AUC-ROC de ~0.87

---

## üìä Visualizaciones Generadas

<details>
<summary>üìà Click para ver las visualizaciones</summary>

### Fase SAMPLE
- **01_sample_distribution.png**: Distribuci√≥n estratificada de clases (entrenamiento vs validaci√≥n)

### Fase EXPLORE
- **02_explore_missing_values.png**: An√°lisis de valores faltantes por variable
- **03_explore_survival_analysis.png**: Supervivencia por g√©nero, clase y puerto
- **04_explore_correlation_heatmap.png**: Matriz de correlaci√≥n entre variables num√©ricas
- **05_explore_boxplots.png**: Detecci√≥n de outliers (Age, Fare, SibSp)

### Fase MODIFY
- **06_modify_feature_engineering.png**: Impacto de las nuevas variables creadas

### Fase ASSESS
- **07_assess_metrics_comparison.png**: Comparaci√≥n de m√©tricas entre modelos
- **08_assess_confusion_matrices.png**: Matrices de confusi√≥n de los 3 modelos
- **09_assess_roc_curves.png**: Curvas ROC comparativas
- **10_assess_feature_importance.png**: Importancia de variables (Random Forest)

</details>

---

## üîç Variables M√°s Importantes

Seg√∫n el an√°lisis de Random Forest:

1. **Title_encoded** - El t√≠tulo (Mr, Mrs, Miss) es el mejor predictor
2. **Sex_encoded** - El g√©nero sigue siendo crucial
3. **Fare** - El precio del boleto correlaciona con supervivencia
4. **Pclass** - La clase del pasajero
5. **Age** - La edad del pasajero

---

## üöÄ Ejecuci√≥n

### Requisitos
```bash
pip install pandas numpy matplotlib seaborn scikit-learn xgboost joblib jupyter
```

### Ejecutar el Notebook
```bash
cd notebooks
jupyter notebook SEMMA_Titanic_Completo.ipynb
```

### Cargar Modelos Pre-entrenados
```python
import joblib

# Cargar modelo y scaler
model = joblib.load('outputs/modelos/random_forest_model.pkl')
scaler = joblib.load('outputs/modelos/scaler.pkl')

# Hacer predicciones
X_new_scaled = scaler.transform(X_new)
predictions = model.predict(X_new_scaled)
```

---

## üìö Conclusiones

1. ‚úÖ La metodolog√≠a **SEMMA** proporciona un marco estructurado y reproducible para proyectos de ML.
2. ‚úÖ El **feature engineering** (Title, FamilySize, IsAlone) mejor√≥ significativamente los modelos.
3. ‚úÖ **Random Forest optimizado** logr√≥ el mejor rendimiento con AUC-ROC de ~0.87.
4. ‚úÖ Los resultados confirman patrones hist√≥ricos: prioridad a mujeres, ni√±os y clases altas.
5. ‚úÖ Todos los modelos est√°n listos para producci√≥n y exportados como archivos `.pkl`.

---

##  Autor

**Equipo Pr√°ctico de Ciencias de Datos  UNACH**

---

<p align="center">
  <i>‚≠ê Si este proyecto te fue √∫til, considera darle una estrella!</i>
</p>

