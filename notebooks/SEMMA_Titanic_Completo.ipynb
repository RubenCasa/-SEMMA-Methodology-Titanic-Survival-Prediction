{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üö¢ Metodolog√≠a SEMMA: Caso Pr√°ctico con Dataset Titanic\n",
    "\n",
    "## Investigaci√≥n: Aplicaci√≥n de la Metodolog√≠a SEMMA en un Caso Pr√°ctico\n",
    "\n",
    "**Dataset:** Titanic: Machine Learning from Disaster (Kaggle)\n",
    "\n",
    "**Autores:** Equipo Pr√°ctico\n",
    "\n",
    "---\n",
    "\n",
    "### Contenido:\n",
    "1. **SAMPLE** - Muestreo y preparaci√≥n de datos\n",
    "2. **EXPLORE** - Exploraci√≥n y an√°lisis descriptivo\n",
    "3. **MODIFY** - Transformaci√≥n y preparaci√≥n\n",
    "4. **MODEL** - Modelado con algoritmos ML\n",
    "5. **ASSESS** - Evaluaci√≥n y comparaci√≥n de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Importaci√≥n de Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: No se puede encontrar el m√≥dulo especificado.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: No se puede encontrar el m√≥dulo especificado."
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: No se puede encontrar el m√≥dulo especificado.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: No se puede encontrar el m√≥dulo especificado."
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Librer√≠as b√°sicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, classification_report, roc_curve, auc,\n",
    "                             roc_auc_score)\n",
    "\n",
    "# XGBoost\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è XGBoost no instalado. Instalando...\")\n",
    "    !pip install xgboost -q\n",
    "    from xgboost import XGBClassifier\n",
    "    XGBOOST_AVAILABLE = True\n",
    "\n",
    "print(\" Todas las librer√≠as cargadas correctamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1Ô∏è‚É£ SAMPLE - Muestreo\n",
    "\n",
    "**Objetivo:** Seleccionar y preparar una muestra representativa del dataset.\n",
    "\n",
    "**Acciones:**\n",
    "- Carga del dataset completo (train.csv con 891 registros)\n",
    "- Divisi√≥n en conjuntos de entrenamiento (80%) y validaci√≥n (20%)\n",
    "- Verificaci√≥n de distribuciones para asegurar representatividad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/titanic.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2321630559.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Cargar el dataset Titanic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/titanic.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Informaci√≥n b√°sica del dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/titanic.csv'"
     ]
    }
   ],
   "source": [
    "# Cargar el dataset Titanic\n",
    "df = pd.read_csv('../data/titanic.csv')\n",
    "\n",
    "# Informaci√≥n b√°sica del dataset\n",
    "print(\"=\"*60)\n",
    "print(\"üìä INFORMACI√ìN DEL DATASET TITANIC\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìÅ Dimensiones: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n",
    "print(f\"\\nüìã Columnas disponibles:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"   {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar primeras filas\n",
    "print(\"\\nüîç Primeras 5 filas del dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n de tipos de datos\n",
    "print(\"\\nüìù Tipos de datos:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisi√≥n del dataset en entrenamiento y validaci√≥n\n",
    "# Usamos estratificaci√≥n para mantener la proporci√≥n de sobrevivientes\n",
    "\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y  # Mantener proporci√≥n de clases\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä DIVISI√ìN DEL DATASET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüîπ Conjunto de Entrenamiento: {len(X_train)} registros ({len(X_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"üîπ Conjunto de Validaci√≥n: {len(X_val)} registros ({len(X_val)/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar distribuci√≥n de la variable objetivo\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Dataset completo\n",
    "axes[0].pie(y.value_counts(), labels=['No Sobrevivi√≥', 'Sobrevivi√≥'], \n",
    "            autopct='%1.1f%%', colors=['#ff6b6b', '#4ecdc4'], explode=[0, 0.05])\n",
    "axes[0].set_title('Dataset Completo\\n(891 registros)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Entrenamiento\n",
    "axes[1].pie(y_train.value_counts(), labels=['No Sobrevivi√≥', 'Sobrevivi√≥'], \n",
    "            autopct='%1.1f%%', colors=['#ff6b6b', '#4ecdc4'], explode=[0, 0.05])\n",
    "axes[1].set_title(f'Entrenamiento\\n({len(y_train)} registros)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Validaci√≥n\n",
    "axes[2].pie(y_val.value_counts(), labels=['No Sobrevivi√≥', 'Sobrevivi√≥'], \n",
    "            autopct='%1.1f%%', colors=['#ff6b6b', '#4ecdc4'], explode=[0, 0.05])\n",
    "axes[2].set_title(f'Validaci√≥n\\n({len(y_val)} registros)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.suptitle('üìä Verificaci√≥n de Distribuci√≥n Estratificada', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/graficos/01_sample_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ SAMPLE completado: Distribuci√≥n estratificada verificada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2Ô∏è‚É£ EXPLORE - Exploraci√≥n\n",
    "\n",
    "**Objetivo:** Analizar caracter√≠sticas, distribuciones y relaciones entre variables.\n",
    "\n",
    "**Acciones:**\n",
    "- An√°lisis descriptivo: media, desviaci√≥n, frecuencias\n",
    "- Visualizaciones: histogramas, boxplots, heatmaps de correlaci√≥n\n",
    "- Identificaci√≥n de valores at√≠picos y valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad√≠sticas descriptivas\n",
    "print(\"=\"*60)\n",
    "print(\"üìä ESTAD√çSTICAS DESCRIPTIVAS\")\n",
    "print(\"=\"*60)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de valores faltantes\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç AN√ÅLISIS DE VALORES FALTANTES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "missing_data = pd.DataFrame({\n",
    "    'Total Faltantes': df.isnull().sum(),\n",
    "    'Porcentaje (%)': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "missing_data = missing_data[missing_data['Total Faltantes'] > 0].sort_values('Total Faltantes', ascending=False)\n",
    "print(missing_data)\n",
    "\n",
    "# Visualizaci√≥n de valores faltantes\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = ['#ff6b6b' if x > 50 else '#ffd93d' if x > 10 else '#6bcb77' for x in missing_data['Porcentaje (%)']]\n",
    "bars = ax.barh(missing_data.index, missing_data['Porcentaje (%)'], color=colors, edgecolor='black')\n",
    "ax.set_xlabel('Porcentaje de Valores Faltantes (%)', fontsize=12)\n",
    "ax.set_title('üîç An√°lisis de Valores Faltantes en el Dataset', fontsize=14, fontweight='bold')\n",
    "\n",
    "for bar, val in zip(bars, missing_data['Porcentaje (%)']):\n",
    "    ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "            f'{val:.1f}%', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/graficos/02_explore_missing_values.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuci√≥n de supervivencia por variables categ√≥ricas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Por Sexo\n",
    "sns.countplot(data=df, x='Sex', hue='Survived', ax=axes[0,0], palette=['#ff6b6b', '#4ecdc4'])\n",
    "axes[0,0].set_title('Supervivencia por Sexo', fontsize=14, fontweight='bold')\n",
    "axes[0,0].legend(['No Sobrevivi√≥', 'Sobrevivi√≥'])\n",
    "\n",
    "# Por Clase\n",
    "sns.countplot(data=df, x='Pclass', hue='Survived', ax=axes[0,1], palette=['#ff6b6b', '#4ecdc4'])\n",
    "axes[0,1].set_title('Supervivencia por Clase', fontsize=14, fontweight='bold')\n",
    "axes[0,1].legend(['No Sobrevivi√≥', 'Sobrevivi√≥'])\n",
    "\n",
    "# Por Puerto de Embarque\n",
    "sns.countplot(data=df, x='Embarked', hue='Survived', ax=axes[1,0], palette=['#ff6b6b', '#4ecdc4'])\n",
    "axes[1,0].set_title('Supervivencia por Puerto de Embarque', fontsize=14, fontweight='bold')\n",
    "axes[1,0].legend(['No Sobrevivi√≥', 'Sobrevivi√≥'])\n",
    "\n",
    "# Distribuci√≥n de Edades por Supervivencia\n",
    "df[df['Survived']==0]['Age'].hist(ax=axes[1,1], alpha=0.6, label='No Sobrevivi√≥', color='#ff6b6b', bins=30)\n",
    "df[df['Survived']==1]['Age'].hist(ax=axes[1,1], alpha=0.6, label='Sobrevivi√≥', color='#4ecdc4', bins=30)\n",
    "axes[1,1].set_title('Distribuci√≥n de Edad por Supervivencia', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Edad')\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.suptitle('üìä EXPLORACI√ìN: An√°lisis de Supervivencia', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/graficos/03_explore_survival_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap de correlaci√≥n\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Seleccionar solo columnas num√©ricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "            cmap='RdYlGn', center=0, linewidths=0.5,\n",
    "            annot_kws={'size': 10})\n",
    "\n",
    "plt.title('üî• Mapa de Correlaci√≥n de Variables Num√©ricas', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/graficos/04_explore_correlation_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ EXPLORE completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots para detectar outliers\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Age\n",
    "sns.boxplot(data=df, y='Age', ax=axes[0], color='#4ecdc4')\n",
    "axes[0].set_title('Distribuci√≥n de Edad', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Fare\n",
    "sns.boxplot(data=df, y='Fare', ax=axes[1], color='#ff6b6b')\n",
    "axes[1].set_title('Distribuci√≥n de Tarifa', fontsize=14, fontweight='bold')\n",
    "\n",
    "# SibSp + Parch\n",
    "sns.boxplot(data=df, y='SibSp', ax=axes[2], color='#ffd93d')\n",
    "axes[2].set_title('Hermanos/Esposos a Bordo', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.suptitle('üì¶ Detecci√≥n de Valores At√≠picos (Outliers)', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/graficos/05_explore_boxplots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3Ô∏è‚É£ MODIFY - Modificaci√≥n\n",
    "\n",
    "**Objetivo:** Transformar y preparar los datos para el modelado.\n",
    "\n",
    "**Acciones:**\n",
    "- Imputaci√≥n de valores faltantes (mediana para Age, moda para Embarked)\n",
    "- Creaci√≥n de nuevas variables: FamilySize, Title, IsAlone\n",
    "- Codificaci√≥n de variables categ√≥ricas (Sex, Embarked)\n",
    "- Normalizaci√≥n de variables num√©ricas (Age, Fare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una copia del dataset para modificar\n",
    "df_modified = df.copy()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîß MODIFICACI√ìN DE DATOS\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. IMPUTACI√ìN DE VALORES FALTANTES\n",
    "print(\"\\nüìå 1. Imputaci√≥n de Valores Faltantes\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Age: Imputar con la mediana\n",
    "age_median = df_modified['Age'].median()\n",
    "df_modified['Age'].fillna(age_median, inplace=True)\n",
    "print(f\"   ‚úì Age: Imputado con mediana = {age_median:.1f}\")\n",
    "\n",
    "# Embarked: Imputar con la moda\n",
    "embarked_mode = df_modified['Embarked'].mode()[0]\n",
    "df_modified['Embarked'].fillna(embarked_mode, inplace=True)\n",
    "print(f\"   ‚úì Embarked: Imputado con moda = '{embarked_mode}'\")\n",
    "\n",
    "# Cabin: Tiene muchos nulos, crear variable indicadora\n",
    "df_modified['HasCabin'] = df_modified['Cabin'].notna().astype(int)\n",
    "print(f\"   ‚úì Cabin: Creada variable indicadora 'HasCabin'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. FEATURE ENGINEERING - Crear nuevas variables\n",
    "print(\"\\nüìå 2. Feature Engineering - Nuevas Variables\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# FamilySize: SibSp + Parch + 1 (el pasajero mismo)\n",
    "df_modified['FamilySize'] = df_modified['SibSp'] + df_modified['Parch'] + 1\n",
    "print(f\"   ‚úì FamilySize creada (SibSp + Parch + 1)\")\n",
    "\n",
    "# IsAlone: Si viaja solo\n",
    "df_modified['IsAlone'] = (df_modified['FamilySize'] == 1).astype(int)\n",
    "print(f\"   ‚úì IsAlone creada (1 si viaja solo, 0 si tiene familia)\")\n",
    "\n",
    "# Title: Extraer del nombre\n",
    "df_modified['Title'] = df_modified['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "# Agrupar t√≠tulos poco comunes\n",
    "title_mapping = {\n",
    "    'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',\n",
    "    'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare',\n",
    "    'Mlle': 'Miss', 'Countess': 'Rare', 'Ms': 'Miss', 'Lady': 'Rare',\n",
    "    'Jonkheer': 'Rare', 'Don': 'Rare', 'Mme': 'Mrs', 'Capt': 'Rare',\n",
    "    'Sir': 'Rare', 'Dona': 'Rare'\n",
    "}\n",
    "df_modified['Title'] = df_modified['Title'].map(title_mapping)\n",
    "df_modified['Title'].fillna('Rare', inplace=True)\n",
    "print(f\"   ‚úì Title extra√≠do del nombre y agrupado\")\n",
    "\n",
    "# Mostrar distribuci√≥n de t√≠tulos\n",
    "print(f\"\\n   Distribuci√≥n de T√≠tulos:\")\n",
    "print(df_modified['Title'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CODIFICACI√ìN DE VARIABLES CATEG√ìRICAS\n",
    "print(\"\\nüìå 3. Codificaci√≥n de Variables Categ√≥ricas\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Label Encoding para Sex\n",
    "le_sex = LabelEncoder()\n",
    "df_modified['Sex_encoded'] = le_sex.fit_transform(df_modified['Sex'])\n",
    "print(f\"   ‚úì Sex: {dict(zip(le_sex.classes_, le_sex.transform(le_sex.classes_)))}\")\n",
    "\n",
    "# Label Encoding para Embarked\n",
    "le_embarked = LabelEncoder()\n",
    "df_modified['Embarked_encoded'] = le_embarked.fit_transform(df_modified['Embarked'])\n",
    "print(f\"   ‚úì Embarked: {dict(zip(le_embarked.classes_, le_embarked.transform(le_embarked.classes_)))}\")\n",
    "\n",
    "# Label Encoding para Title\n",
    "le_title = LabelEncoder()\n",
    "df_modified['Title_encoded'] = le_title.fit_transform(df_modified['Title'])\n",
    "print(f\"   ‚úì Title: {dict(zip(le_title.classes_, le_title.transform(le_title.classes_)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. SELECCI√ìN DE FEATURES FINALES\n",
    "print(\"\\nüìå 4. Selecci√≥n de Features para Modelado\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Features seleccionadas\n",
    "feature_columns = ['Pclass', 'Sex_encoded', 'Age', 'SibSp', 'Parch', 'Fare',\n",
    "                   'Embarked_encoded', 'FamilySize', 'IsAlone', 'HasCabin', 'Title_encoded']\n",
    "\n",
    "X = df_modified[feature_columns]\n",
    "y = df_modified['Survived']\n",
    "\n",
    "print(f\"   Features seleccionadas ({len(feature_columns)}):\")\n",
    "for i, col in enumerate(feature_columns, 1):\n",
    "    print(f\"   {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. NORMALIZACI√ìN\n",
    "print(\"\\nüìå 5. Normalizaci√≥n de Variables Num√©ricas\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Divisi√≥n de datos\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Normalizaci√≥n con StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Convertir a DataFrame para mejor visualizaci√≥n\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_columns)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=feature_columns)\n",
    "\n",
    "print(f\"   ‚úì StandardScaler aplicado a todas las features\")\n",
    "print(f\"   ‚úì Media ‚âà 0, Desviaci√≥n Est√°ndar ‚âà 1\")\n",
    "\n",
    "print(\"\\n‚úÖ MODIFY completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n del Feature Engineering\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# FamilySize vs Survived\n",
    "sns.barplot(data=df_modified, x='FamilySize', y='Survived', ax=axes[0], palette='viridis')\n",
    "axes[0].set_title('Tasa de Supervivencia por Tama√±o de Familia', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Tama√±o de Familia')\n",
    "axes[0].set_ylabel('Tasa de Supervivencia')\n",
    "\n",
    "# IsAlone vs Survived\n",
    "sns.barplot(data=df_modified, x='IsAlone', y='Survived', ax=axes[1], palette=['#4ecdc4', '#ff6b6b'])\n",
    "axes[1].set_title('Supervivencia: Solo vs Con Familia', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xticklabels(['Con Familia', 'Viaja Solo'])\n",
    "axes[1].set_ylabel('Tasa de Supervivencia')\n",
    "\n",
    "# Title vs Survived\n",
    "sns.barplot(data=df_modified, x='Title', y='Survived', ax=axes[2], palette='Set2')\n",
    "axes[2].set_title('Supervivencia por T√≠tulo', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('T√≠tulo')\n",
    "axes[2].set_ylabel('Tasa de Supervivencia')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.suptitle('üîß MODIFY: Impacto de las Nuevas Variables', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/graficos/06_modify_feature_engineering.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4Ô∏è‚É£ MODEL - Modelado\n",
    "\n",
    "**Objetivo:** Aplicar algoritmos de aprendizaje autom√°tico para predecir la supervivencia.\n",
    "\n",
    "**Modelos a implementar:**\n",
    "1. Regresi√≥n Log√≠stica (baseline)\n",
    "2. Random Forest (con ajuste de hiperpar√°metros)\n",
    "3. XGBoost (Gradient Boosting)\n",
    "\n",
    "**Validaci√≥n:** Cross-validation con 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario para almacenar resultados\n",
    "results = {}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ü§ñ MODELADO\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELO 1: Regresi√≥n Log√≠stica (Baseline)\n",
    "print(\"\\nüìå Modelo 1: Regresi√≥n Log√≠stica (Baseline)\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_lr = cross_val_score(lr_model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"   Cross-Validation Accuracy: {cv_scores_lr.mean():.4f} (+/- {cv_scores_lr.std()*2:.4f})\")\n",
    "\n",
    "# Predicciones\n",
    "y_pred_lr = lr_model.predict(X_val_scaled)\n",
    "y_prob_lr = lr_model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "results['Logistic Regression'] = {\n",
    "    'model': lr_model,\n",
    "    'y_pred': y_pred_lr,\n",
    "    'y_prob': y_prob_lr,\n",
    "    'cv_scores': cv_scores_lr\n",
    "}\n",
    "\n",
    "print(\"   ‚úì Modelo entrenado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELO 2: Random Forest con GridSearchCV\n",
    "print(\"\\nüìå Modelo 2: Random Forest (con tuning)\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Definir grid de hiperpar√°metros\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_grid = GridSearchCV(rf_model, rf_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "rf_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"   Mejores par√°metros: {rf_grid.best_params_}\")\n",
    "print(f\"   Mejor CV Accuracy: {rf_grid.best_score_:.4f}\")\n",
    "\n",
    "# Usar mejor modelo\n",
    "rf_best = rf_grid.best_estimator_\n",
    "\n",
    "# Cross-validation con mejor modelo\n",
    "cv_scores_rf = cross_val_score(rf_best, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Predicciones\n",
    "y_pred_rf = rf_best.predict(X_val_scaled)\n",
    "y_prob_rf = rf_best.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "results['Random Forest'] = {\n",
    "    'model': rf_best,\n",
    "    'y_pred': y_pred_rf,\n",
    "    'y_prob': y_prob_rf,\n",
    "    'cv_scores': cv_scores_rf\n",
    "}\n",
    "\n",
    "print(\"   ‚úì Modelo entrenado con tuning completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELO 3: XGBoost\n",
    "print(\"\\nüìå Modelo 3: XGBoost\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_xgb = cross_val_score(xgb_model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"   Cross-Validation Accuracy: {cv_scores_xgb.mean():.4f} (+/- {cv_scores_xgb.std()*2:.4f})\")\n",
    "\n",
    "# Predicciones\n",
    "y_pred_xgb = xgb_model.predict(X_val_scaled)\n",
    "y_prob_xgb = xgb_model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "results['XGBoost'] = {\n",
    "    'model': xgb_model,\n",
    "    'y_pred': y_pred_xgb,\n",
    "    'y_prob': y_prob_xgb,\n",
    "    'cv_scores': cv_scores_xgb\n",
    "}\n",
    "\n",
    "print(\"   ‚úì Modelo entrenado correctamente\")\n",
    "print(\"\\n‚úÖ MODEL completado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5Ô∏è‚É£ ASSESS - Evaluaci√≥n\n",
    "\n",
    "**Objetivo:** Evaluar y comparar el rendimiento de los modelos.\n",
    "\n",
    "**M√©tricas:**\n",
    "- Accuracy (Precisi√≥n global)\n",
    "- Precision (Precisi√≥n para clase positiva)\n",
    "- Recall (Sensibilidad)\n",
    "- F1-Score\n",
    "- AUC-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üìä EVALUACI√ìN DE MODELOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calcular m√©tricas para cada modelo\n",
    "metrics_summary = []\n",
    "\n",
    "for model_name, model_data in results.items():\n",
    "    y_pred = model_data['y_pred']\n",
    "    y_prob = model_data['y_prob']\n",
    "    \n",
    "    metrics = {\n",
    "        'Modelo': model_name,\n",
    "        'Accuracy': accuracy_score(y_val, y_pred),\n",
    "        'Precision': precision_score(y_val, y_pred),\n",
    "        'Recall': recall_score(y_val, y_pred),\n",
    "        'F1-Score': f1_score(y_val, y_pred),\n",
    "        'AUC-ROC': roc_auc_score(y_val, y_prob),\n",
    "        'CV Mean': model_data['cv_scores'].mean(),\n",
    "        'CV Std': model_data['cv_scores'].std()\n",
    "    }\n",
    "    metrics_summary.append(metrics)\n",
    "\n",
    "# Crear DataFrame de resultados\n",
    "metrics_df = pd.DataFrame(metrics_summary)\n",
    "metrics_df = metrics_df.set_index('Modelo')\n",
    "\n",
    "# Formatear para visualizaci√≥n\n",
    "print(\"\\nüìã Tabla Comparativa de Modelos:\")\n",
    "print(metrics_df.round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n comparativa de m√©tricas\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gr√°fico de barras de m√©tricas\n",
    "metrics_plot = metrics_df[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']]\n",
    "metrics_plot.plot(kind='bar', ax=axes[0], width=0.8, edgecolor='black')\n",
    "axes[0].set_title('üìä Comparaci√≥n de M√©tricas por Modelo', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "for container in axes[0].containers:\n",
    "    axes[0].bar_label(container, fmt='%.2f', fontsize=8)\n",
    "\n",
    "# Cross-validation scores\n",
    "cv_data = {\n",
    "    'Modelo': list(results.keys()),\n",
    "    'CV Score': [r['cv_scores'].mean() for r in results.values()],\n",
    "    'CV Std': [r['cv_scores'].std() for r in results.values()]\n",
    "}\n",
    "cv_df = pd.DataFrame(cv_data)\n",
    "\n",
    "colors = ['#4ecdc4', '#ff6b6b', '#ffd93d']\n",
    "bars = axes[1].bar(cv_df['Modelo'], cv_df['CV Score'], yerr=cv_df['CV Std']*2, \n",
    "                    capsize=5, color=colors, edgecolor='black')\n",
    "axes[1].set_title('üìà Cross-Validation Accuracy (5-Fold)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_ylim(0.7, 0.9)\n",
    "\n",
    "for bar, score in zip(bars, cv_df['CV Score']):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                 f'{score:.4f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/graficos/07_assess_metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices de Confusi√≥n\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, (model_name, model_data) in enumerate(results.items()):\n",
    "    cm = confusion_matrix(y_val, model_data['y_pred'])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['No Sobrevivi√≥', 'Sobrevivi√≥'],\n",
    "                yticklabels=['No Sobrevivi√≥', 'Sobrevivi√≥'],\n",
    "                annot_kws={'size': 14})\n",
    "    axes[idx].set_title(f'{model_name}', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Predicci√≥n')\n",
    "    axes[idx].set_ylabel('Real')\n",
    "\n",
    "plt.suptitle('üìä Matrices de Confusi√≥n', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/graficos/08_assess_confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curvas ROC\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors = {'Logistic Regression': '#4ecdc4', 'Random Forest': '#ff6b6b', 'XGBoost': '#ffd93d'}\n",
    "\n",
    "for model_name, model_data in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_val, model_data['y_prob'])\n",
    "    auc_score = roc_auc_score(y_val, model_data['y_prob'])\n",
    "    \n",
    "    plt.plot(fpr, tpr, color=colors[model_name], linewidth=2,\n",
    "             label=f'{model_name} (AUC = {auc_score:.3f})')\n",
    "\n",
    "# L√≠nea de referencia\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier (AUC = 0.5)')\n",
    "\n",
    "plt.xlabel('Tasa de Falsos Positivos (FPR)', fontsize=12)\n",
    "plt.ylabel('Tasa de Verdaderos Positivos (TPR)', fontsize=12)\n",
    "plt.title('üìà Curvas ROC - Comparaci√≥n de Modelos', fontsize=16, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/graficos/09_assess_roc_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (Random Forest)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Obtener importancia de features del Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Importance': rf_best.feature_importances_\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(feature_importance)))\n",
    "bars = plt.barh(feature_importance['Feature'], feature_importance['Importance'], \n",
    "                color=colors, edgecolor='black')\n",
    "\n",
    "plt.xlabel('Importancia', fontsize=12)\n",
    "plt.title('üîç Importancia de Variables (Random Forest)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for bar, imp in zip(bars, feature_importance['Importance']):\n",
    "    plt.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "             f'{imp:.3f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/graficos/10_assess_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ ASSESS completado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üìù Resumen y Conclusiones\n",
    "\n",
    "## Resumen de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen final\n",
    "print(\"=\"*70)\n",
    "print(\"üìä RESUMEN FINAL - METODOLOG√çA SEMMA APLICADA AL TITANIC DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìå ETAPAS COMPLETADAS:\")\n",
    "print(\"-\"*40)\n",
    "print(\"‚úÖ SAMPLE: Dataset dividido 80/20 con estratificaci√≥n\")\n",
    "print(\"‚úÖ EXPLORE: An√°lisis descriptivo y visualizaciones completadas\")\n",
    "print(\"‚úÖ MODIFY: Feature engineering y preprocesamiento aplicados\")\n",
    "print(\"‚úÖ MODEL: 3 modelos entrenados con cross-validation\")\n",
    "print(\"‚úÖ ASSESS: Evaluaci√≥n completa con m√∫ltiples m√©tricas\")\n",
    "\n",
    "print(\"\\nüìà RESULTADOS DE LOS MODELOS:\")\n",
    "print(\"-\"*40)\n",
    "print(metrics_df[['Accuracy', 'AUC-ROC', 'CV Mean']].round(4).to_string())\n",
    "\n",
    "# Mejor modelo\n",
    "best_model = metrics_df['AUC-ROC'].idxmax()\n",
    "best_auc = metrics_df.loc[best_model, 'AUC-ROC']\n",
    "\n",
    "print(f\"\\nüèÜ MEJOR MODELO: {best_model}\")\n",
    "print(f\"   AUC-ROC: {best_auc:.4f}\")\n",
    "print(f\"   Accuracy: {metrics_df.loc[best_model, 'Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "### Hallazgos Principales:\n",
    "\n",
    "1. **SAMPLE:** La estratificaci√≥n garantiz√≥ que las proporciones de supervivientes se mantuvieran en ambos conjuntos (entrenamiento y validaci√≥n).\n",
    "\n",
    "2. **EXPLORE:** \n",
    "   - Las mujeres tuvieron mayor tasa de supervivencia que los hombres\n",
    "   - Los pasajeros de primera clase sobrevivieron m√°s\n",
    "   - La variable Age ten√≠a ~20% de valores faltantes\n",
    "\n",
    "3. **MODIFY:**\n",
    "   - El feature engineering (FamilySize, Title, IsAlone) mejor√≥ la predicci√≥n\n",
    "   - La codificaci√≥n y normalizaci√≥n prepararon los datos correctamente\n",
    "\n",
    "4. **MODEL:**\n",
    "   - Random Forest y XGBoost superaron a la Regresi√≥n Log√≠stica\n",
    "   - El tuning de hiperpar√°metros mejor√≥ el rendimiento\n",
    "\n",
    "5. **ASSESS:**\n",
    "   - El mejor modelo logr√≥ un AUC-ROC superior a 0.85\n",
    "   - Las variables m√°s importantes fueron: Title, Sex, Fare y Pclass\n",
    "\n",
    "### Recomendaciones:\n",
    "- Probar t√©cnicas de balanceo de clases (SMOTE)\n",
    "- Explorar ensemble de modelos\n",
    "- Implementar MLOps para producci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelos\n",
    "import joblib\n",
    "\n",
    "for model_name, model_data in results.items():\n",
    "    filename = f\"../outputs/modelos/{model_name.replace(' ', '_').lower()}_model.pkl\"\n",
    "    joblib.dump(model_data['model'], filename)\n",
    "    print(f\"‚úÖ Modelo guardado: {filename}\")\n",
    "\n",
    "# Guardar scaler\n",
    "joblib.dump(scaler, '../outputs/modelos/scaler.pkl')\n",
    "print(\"‚úÖ Scaler guardado: ../outputs/modelos/scaler.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ PROYECTO SEMMA COMPLETADO EXITOSAMENTE!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
